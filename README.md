# ğŸ–ï¸ Real-Time Sign Language Recognition using MediaPipe & Neural Networks

ğŸ“Œ **Overview**

This project bridges the communication gap between sign language users and non-users by recognizing hand gestures in real time using hand landmark detection and machine learning. Leveraging MediaPipe by Google and a trained Neural Network, the system identifies sign language gestures from webcam input and displays the recognized text on the screen â€” enabling a seamless, interactive communication experience.


ğŸ¯ **Features**

â€¢ğŸ” Real-time hand gesture recognition via webcam

â€¢ğŸ“ 21-point hand landmark detection using MediaPipe

â€¢ğŸ¤– Gesture classification using a trained Neural Network

â€¢ğŸ“Š Dataset generated and stored in CSV format for training

â€¢ğŸ“º Live prediction display on screen


ğŸ› ï¸ **Tech Stack**

â€¢MediaPipe â€“ for real-time hand landmark detection

â€¢OpenCV â€“ for webcam frame capture and processing

â€¢Python â€“ core programming language

â€¢NumPy & Pandas â€“ for data manipulation

â€¢TensorFlow/Keras â€“ to build and train the neural network


ğŸ§  **How It Works**

1)Landmark Detection
MediaPipe extracts 21 hand landmarks per frame (x, y, z coordinates).

2)Dataset Creation
Thousands of samples per gesture are captured and saved as rows in a CSV file.

3)Model Training
A neural network is trained on this structured dataset to classify gestures accurately.

4)Real-Time Prediction
Webcam input is analyzed frame by frame, with predicted gesture text shown on the screen.
